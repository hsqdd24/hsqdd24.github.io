<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>这是一篇新的博文 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="魔搭入门一、引言大部分内容参考：魔搭 ModelScope 介绍和使用实录 (一)魔搭官方文档 二、miniconda环境准备在开始之前需要安装miniconda(考虑到服务器内存原因,先安装miniconda,而不是Anaconda,需要什么包再下载就行) 这部分参考:管理学院服务器 R-servers 登录方法_通用管理学院服务器指南miniconda 安装及使用 VScode配置服务器及mi">
<meta property="og:type" content="article">
<meta property="og:title" content="这是一篇新的博文">
<meta property="og:url" content="http://example.com/2025/03/14/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E6%96%B0%E7%9A%84%E5%8D%9A%E6%96%87/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="魔搭入门一、引言大部分内容参考：魔搭 ModelScope 介绍和使用实录 (一)魔搭官方文档 二、miniconda环境准备在开始之前需要安装miniconda(考虑到服务器内存原因,先安装miniconda,而不是Anaconda,需要什么包再下载就行) 这部分参考:管理学院服务器 R-servers 登录方法_通用管理学院服务器指南miniconda 安装及使用 VScode配置服务器及mi">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-03-14T11:43:00.000Z">
<meta property="article:modified_time" content="2025-03-14T11:54:41.596Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-这是一篇新的博文" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/03/14/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E6%96%B0%E7%9A%84%E5%8D%9A%E6%96%87/" class="article-date">
  <time class="dt-published" datetime="2025-03-14T11:43:00.000Z" itemprop="datePublished">2025-03-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      这是一篇新的博文
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="魔搭入门"><a href="#魔搭入门" class="headerlink" title="魔搭入门"></a>魔搭入门</h1><h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>大部分内容参考：<br><a target="_blank" rel="noopener" href="https://yeyeziblog.eu.org/2024/11/05/Tech/LLM/modelscope1/">魔搭 ModelScope 介绍和使用实录 (一)</a><br><a target="_blank" rel="noopener" href="https://www.modelscope.cn/docs/intro/quickstart">魔搭官方文档</a></p>
<h2 id="二、miniconda环境准备"><a href="#二、miniconda环境准备" class="headerlink" title="二、miniconda环境准备"></a>二、miniconda环境准备</h2><p>在开始之前需要安装<code>miniconda</code>(考虑到服务器内存原因,先安装<code>miniconda</code>,而不是<code>Anaconda</code>,需要什么包再下载就行)</p>
<p>这部分参考:<br><a target="_blank" rel="noopener" href="http://10.9.65.31/2023/06/07/%E7%AE%A1%E7%90%86%E5%AD%A6%E9%99%A2/%E7%AE%A1%E7%90%86%E5%AD%A6%E9%99%A2%E6%9C%8D%E5%8A%A1%E5%99%A8R-servers%E7%99%BB%E5%BD%95%E6%96%B9%E6%B3%95_%E9%80%9A%E7%94%A8/">管理学院服务器 R-servers 登录方法_通用</a><br><a target="_blank" rel="noopener" href="http://10.9.65.31/2022/05/15/%E7%AE%A1%E7%90%86%E5%AD%A6%E9%99%A2/%E7%AE%A1%E7%90%86%E5%AD%A6%E9%99%A2%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/">管理学院服务器指南</a><br><a target="_blank" rel="noopener" href="https://yeyeziblog.eu.org/2023/05/24/Tech/conda/miniconda%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8/">miniconda 安装及使用</a></p>
<h3 id="VScode配置服务器及miniconda环境准备"><a href="#VScode配置服务器及miniconda环境准备" class="headerlink" title="VScode配置服务器及miniconda环境准备"></a>VScode配置服务器及miniconda环境准备</h3><p><strong>VScode配置服务器:</strong></p>
<ol>
<li>打开VScode,在左边扩展里搜索“Remote - SSH”插件,安装完成后,扩展下面会出现一个“远程资源管理器”,点击</li>
<li>在出现的“远程(隧道&#x2F;SSH)”下面有一个SSH,鼠标拖到上面，右边会出现一个设置符号和一个+号,点击设置符号(打开SSH配置文件),选择最上面的“C:\Users\HP\.ssh\config”</li>
<li>在config文件里面根据它的格式,改自己的Host(例如 Host:YL  HostName:10.9.65.32  User:bdcd_yl),保存</li>
<li>回到左边,刷新一下,出现了.点击”YL”的右箭头,输入密码确认,之后点确定、允许之类的一步步操作,就连好了</li>
<li>然后在”资源管理器”里面”打开文件夹”,确定,再输入一遍密码确定.在右上角菜单栏里点击三个点-终端-新建终端,即进入了bash环境</li>
<li>如果要关闭,点左下角绿色的”SSH:YL”,选择最下面的”关闭远程连接”即可</li>
</ol>
<p><strong>miniconda环境准备使用了以下代码:</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line">bash ./Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line"></span><br><span class="line"><span class="comment">## Conda-forge</span></span><br><span class="line">conda config --add channels conda-forge</span><br><span class="line">conda config --<span class="built_in">set</span> channel_priority strict</span><br><span class="line"></span><br><span class="line">conda create -n python3</span><br><span class="line">conda <span class="built_in">env</span> list</span><br><span class="line">conda activate python3</span><br><span class="line">conda install python=3.9 <span class="comment">#python版本不一定用最新的，否则不一定能安装成功</span></span><br><span class="line">conda install auto-sklearn scipy</span><br></pre></td></tr></table></figure>

<p>conda的一些命令可参考:<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/Stromboli/article/details/142713005">Conda命令详解:高效更新与环境管理指南</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_74738450/article/details/135827553">Conda虚拟环境管理(创建、删除、进入、退出)详细教程</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/422365954">shell脚本中激活conda虚拟环境</a></p>
<h2 id="三、环境准备"><a href="#三、环境准备" class="headerlink" title="三、环境准备"></a>三、环境准备</h2><p>从这里开始的所有代码均在Windows系统的cmd里面运行的,均能运行成功,速度很慢,后面我才换成的服务器运行.代码不会有大的变化,之后要用的时候和<strong>引言</strong>里的教程结合看.</p>
<h3 id="1-Python-环境安装配置"><a href="#1-Python-环境安装配置" class="headerlink" title="(1) Python 环境安装配置"></a>(1) Python 环境安装配置</h3><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create -n modelscope python=<span class="number">3</span>.<span class="number">12</span></span><br><span class="line">conda init  #初始化conda环境</span><br><span class="line">conda activate modelscope  #激活conda环境</span><br></pre></td></tr></table></figure>

<h3 id="2-ModelScope-Library-安装"><a href="#2-ModelScope-Library-安装" class="headerlink" title="(2) ModelScope Library 安装"></a>(2) ModelScope Library 安装</h3><ul>
<li>只需要通过 ModelScope SDK,或者 ModelScope 命令行工具来下载模型</li>
</ul>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install modelscope  #安装modelscope</span><br><span class="line">pip install modelscope --upgrade  #更新modelscope</span><br></pre></td></tr></table></figure>

<ul>
<li>需要更完整的使用 ModelScope 平台上的一系列框架能力,包括数据集的加载,外部模型的使用等,则推荐使用”framework”的安装选项</li>
</ul>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install modelscope[framework]</span><br></pre></td></tr></table></figure>

<h3 id="3-深度学习框架依赖的安装"><a href="#3-深度学习框架依赖的安装" class="headerlink" title="(3)深度学习框架依赖的安装"></a>(3)深度学习框架依赖的安装</h3><p>安装PyTorch</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers</span><br><span class="line">pip install --user --upgrade numpy   #将numpy更新到最新版本</span><br><span class="line">pip install torch   #安装PyTorch</span><br><span class="line">pip3 install torch torchvision torchaudio  #安装PyTorch</span><br></pre></td></tr></table></figure>

<p>安装TensorFlow</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow   #安装TensorFlow</span><br><span class="line">pip install --upgrade tensorflow #更新TensorFlow</span><br></pre></td></tr></table></figure>

<h3 id="4-分领域ModelScope模型依赖的安装"><a href="#4-分领域ModelScope模型依赖的安装" class="headerlink" title="(4)分领域ModelScope模型依赖的安装"></a>(4)分领域ModelScope模型依赖的安装</h3><p>如仅需体验NLP领域模型,可执行如下命令安装领域依赖(因部分依赖由ModelScope独立host,所以需要使用”-f”参数):</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install &quot;modelscope[nlp]&quot; -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html</span><br></pre></td></tr></table></figure>

<p>安装成功后,即可使用对应领域模型进行推理,训练等操作。这里我们以NLP领域为例,在”pip install modelscope[nlp]”后，可执行如下命令，运行中文分词任务，来验证安装是否正确:</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install pandas   #先安装pandas</span><br><span class="line">pip install --upgrade pandas</span><br><span class="line"></span><br><span class="line">python -c &quot;from modelscope.pipelines import pipeline;<span class="built_in">print</span>(pipeline(&#x27;word-segmentation&#x27;)(&#x27;世界上有三种尺,直尺,三角尺,还有I love you very much&#x27;))&quot;</span><br></pre></td></tr></table></figure>

<h2 id="四、模型下载"><a href="#四、模型下载" class="headerlink" title="四、模型下载"></a>四、模型下载</h2><p>如果您在高带宽的机器上运行,推荐使用ModelScope命令行工具下载模型.该方法支持断点续传和模型高速下载,例如可以通过如下命令,将Qwen2.5-0.5B-Instruct模型,下载到当前路径下的”model-dir”目录.</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modelscope download --model=&quot;Qwen/Qwen2.<span class="number">5</span>-<span class="number">0</span>.<span class="number">5</span>B-Instruct&quot; --local_dir ./model-<span class="built_in">dir</span></span><br></pre></td></tr></table></figure>

<p>ps.这里虽然更换了安装路径,但我发现后面运行代码的时候,又重新下载了一遍模型到默认文件夹里面.</p>
<h2 id="五、模型加载"><a href="#五、模型加载" class="headerlink" title="五、模型加载"></a>五、模型加载</h2><p>如果模型和ModelScope SDK(即Python)绑定,则只需要几行代码即可加载模型.</p>
<h3 id="1-使用-ModelScope-pipeline-加载模型"><a href="#1-使用-ModelScope-pipeline-加载模型" class="headerlink" title="(1)使用 ModelScope pipeline 加载模型"></a>(1)使用 ModelScope pipeline 加载模型</h3><p>以词性标注模型(damo&#x2F;nlp_structbert_word-segmentation_chinese-base)为例进行简要说明:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope.pipelines <span class="keyword">import</span> pipeline  <span class="comment">#首先实例化一个基于任务的 pipeline 对象</span></span><br><span class="line">word_segmentation = pipeline(<span class="string">&#x27;word-segmentation&#x27;</span>,model=<span class="string">&#x27;damo/nlp_structbert_word-segmentation_chinese-base&#x27;</span>)</span><br><span class="line"></span><br><span class="line">input_str = <span class="string">&#x27;今天天气不错,适合出去游玩&#x27;</span> <span class="comment">#输入数据并获取输出</span></span><br><span class="line"><span class="built_in">print</span>(word_segmentation(input_str))</span><br></pre></td></tr></table></figure>

<p>输出结果为</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;output&#x27;: [&#x27;今天&#x27;, &#x27;天气&#x27;, &#x27;不错&#x27;, &#x27;,&#x27;, &#x27;适合&#x27;, &#x27;出去&#x27;, &#x27;游玩&#x27;]&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-使用-AutoModel-加载模型"><a href="#2-使用-AutoModel-加载模型" class="headerlink" title="(2)使用 AutoModel 加载模型"></a>(2)使用 AutoModel 加载模型</h3><p>先下载更新accelerate</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install accelerate</span><br><span class="line">pip install --upgrade accelerate</span><br></pre></td></tr></table></figure>

<p>再运行下列代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_name,</span><br><span class="line">    torch_dtype=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span></span><br><span class="line">)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br></pre></td></tr></table></figure>

<h2 id="六、模型推理"><a href="#六、模型推理" class="headerlink" title="六、模型推理"></a>六、模型推理</h2><h3 id="1-AutoModel-和-AutoTokenizer"><a href="#1-AutoModel-和-AutoTokenizer" class="headerlink" title="(1)AutoModel 和 AutoTokenizer"></a>(1)AutoModel 和 AutoTokenizer</h3><p>ModelScope兼容了Transformers提供的简单而统一的方法来加载预训练实例和tokenizer.这意味着您可以使用ModelScope加载AutoModel和AutoTokenizer等类.下面是一个大语言模型的完整的运行示例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_name,</span><br><span class="line">    torch_dtype=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span></span><br><span class="line">)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#引号里面的内容可以更改,用英文.更改后要重新跑后面的代码</span></span><br><span class="line">prompt = <span class="string">&quot;Give me a short introduction to large language model.&quot;</span></span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;</span><br><span class="line">]</span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize=<span class="literal">False</span>,</span><br><span class="line">    add_generation_prompt=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">model_inputs = tokenizer([text], return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line"></span><br><span class="line">generated_ids = model.generate(</span><br><span class="line">    **model_inputs,</span><br><span class="line">    max_new_tokens=<span class="number">512</span></span><br><span class="line">)</span><br><span class="line">generated_ids = [</span><br><span class="line">    output_ids[<span class="built_in">len</span>(input_ids):] <span class="keyword">for</span> input_ids, output_ids <span class="keyword">in</span> <span class="built_in">zip</span>(model_inputs.input_ids, generated_ids)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>输入代码<code>print(response)</code>返回结果,注意这里每一次给出的回答不一定一模一样</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Certainly! A large language model (LLM) is a <span class="built_in">type</span> of artificial intelligence that can generate human-like text on its own. These models are designed to mimic the complexity and creativity of human language, allowing them to produce coherent and meaningful responses to a wide range of questions or prompts.</span><br><span class="line"></span><br><span class="line">Large language models differ from traditional machine learning algorithms <span class="keyword">in</span> several ways:</span><br><span class="line"><span class="number">1</span>. **Complexity**: They process vast amounts of data <span class="built_in">at</span> an unprecedented scale.</span><br><span class="line"><span class="number">2</span>. **Variety**: They can handle multiple tasks simultaneously due to their ability to learn from examples.</span><br><span class="line"><span class="number">3</span>. **Creativity**: They have the capacity to generate novel ideas and expressions beyond what could be generated by a human.</span><br><span class="line"><span class="number">4</span>. **Speed**: Their performance is often orders of magnitude faster than human brains when dealing with complex problems.</span><br><span class="line"><span class="number">5</span>. **Accessibility**: They are widely available <span class="keyword">for</span> use through various platforms and tools.</span><br><span class="line"></span><br><span class="line">Examples of LLMs include ChatGPT, Bard, and OpenAI&#x27;s DALL-E <span class="number">3</span>, which have become increasingly popular <span class="keyword">in</span> recent years due to their impressive natural-language generation capabilities. These models have also been used <span class="keyword">in</span> fields such as education, healthcare, legal research, and <span class="built_in">more</span>.</span><br></pre></td></tr></table></figure>

<h3 id="2-pipeline"><a href="#2-pipeline" class="headerlink" title="(2)pipeline"></a>(2)pipeline</h3><h3 id="3-其他模型推理"><a href="#3-其他模型推理" class="headerlink" title="(3)其他模型推理"></a>(3)其他模型推理</h3><p>以上两块不展开说,总结起来就是可以在<strong>模型介绍</strong>里面找到相关代码.<br>此外,这些仅为入门，有更多需求还需认真学习官方文档.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/03/14/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E6%96%B0%E7%9A%84%E5%8D%9A%E6%96%87/" data-id="cm88seyry0000asuwb5u661b9" data-title="这是一篇新的博文" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2025/03/13/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">March 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/03/14/%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E6%96%B0%E7%9A%84%E5%8D%9A%E6%96%87/">这是一篇新的博文</a>
          </li>
        
          <li>
            <a href="/2025/03/13/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>